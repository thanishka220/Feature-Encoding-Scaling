# Task 4: Feature Encoding & Scaling â€“ Adult Income Dataset

## ğŸ“Œ Project Overview
This project focuses on preparing the **Adult Income Dataset** for Machine Learning by applying proper **feature encoding** and **feature scaling** techniques. The goal is to convert raw categorical and numerical data into an ML-ready format and understand the impact of scaling on model performance.

---

## ğŸ“‚ Dataset Information
- **Dataset Name:** Adult Income Dataset
- **Source:** Kaggle  
- **Link:** https://www.kaggle.com/datasets/wenruliu/adult-income-dataset
- **Records:** 48,842 rows
- **Target Variable:** `income`

---

## ğŸ› ï¸ Tools & Technologies Used
- Python  
- Pandas  
- NumPy  
- Matplotlib  
- Seaborn  
- Scikit-learn  

---

## ğŸ“Š Feature Identification

### ğŸ”¹ Categorical Features
- workclass  
- education  
- marital-status  
- occupation  
- relationship  
- race  
- gender  
- native-country  
- income (target)

### ğŸ”¹ Numerical Features
- age  
- fnlwgt  
- educational-num  
- capital-gain  
- capital-loss  
- hours-per-week  

---

## ğŸ”„ Data Preprocessing Steps

### 1ï¸âƒ£ Handling Missing Values
- Replaced `?` with NaN
- Removed rows with missing values

### 2ï¸âƒ£ Feature Encoding
- **Label Encoding** applied to the target variable `income`
  - `<=50K â†’ 0`
  - `>50K â†’ 1`
- **One-Hot Encoding** applied to nominal categorical features using `pd.get_dummies()`
- `drop_first=True` used to avoid multicollinearity

### 3ï¸âƒ£ Feature Scaling
- **StandardScaler** applied to numerical features
- Transformed data to:
  - Mean â‰ˆ 0
  - Standard Deviation â‰ˆ 1

---

## ğŸ“ˆ Before vs After Scaling

### ğŸ”¹ Before Scaling
- Numerical features had different ranges
- Large values dominated distance-based models
- Slower convergence for gradient-based algorithms

### ğŸ”¹ After Scaling
- All numerical features standardized
- Improved model stability and performance
- Suitable for algorithms like KNN, SVM, Logistic Regression, and Neural Networks

---

## ğŸ§  Impact of Scaling on ML Algorithms

### âœ… Scaling is Important For:
- Logistic Regression
- KNN
- SVM
- Neural Networks
- Gradient Descent-based models

### âŒ Scaling Not Mandatory For:
- Decision Trees
- Random Forest
- XGBoost

---

## ğŸ“ Project Files
- `feature encoding.ipynb` â€“ Complete preprocessing notebook  
- `adult_income_preprocessed.zip` â€“ Final processed dataset 

---

## âœ… Final Outcome
- Dataset converted to fully numerical format
- Features properly encoded and scaled
- ML-ready dataset created
- Clear understanding of preprocessing impact on machine learning models

---

## ğŸ“Œ Conclusion
Feature encoding and scaling are crucial preprocessing steps in any machine learning pipeline. This project demonstrates how proper preprocessing improves data quality and model readiness while ensuring fairness across features.
